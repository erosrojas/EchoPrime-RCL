{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import cv2\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only the first few columns (e.g., first 3 columns) from a CSV file\n",
    "df = pd.read_csv('bahar_data_1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of empty reports: 8, patient_indices: [19609, 34167, 55005, 81560, 102207, 122282, 142499, 170474]\n"
     ]
    }
   ],
   "source": [
    "def remove_empty_reports(df):\n",
    "    indices = []\n",
    "\n",
    "    for i, report in enumerate(df['report_page1']):\n",
    "        if pd.isna(report) or report == \"b''\" or report == \"\" or report.strip() == \"\":\n",
    "            indices.append(i)\n",
    "\n",
    "    df.drop(indices, axis=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True) \n",
    "    print(f'no. of empty reports: {len(indices)}, patient_indices: {indices}')\n",
    "\n",
    "remove_empty_reports(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty extractions: 44 Extraction percentage: 0.02382989785639237\n",
      "Bad reports: [2385, 16181, 19888, 20265, 20872, 21786, 22032, 22825, 24556, 24851, 36989, 51432, 55679, 56322, 57300, 57554, 58374, 58375, 60274, 60596, 79225, 82190, 83027, 99678, 100366, 102916, 103851, 108541, 120014, 122902, 123725, 140015, 140691, 143195, 144117, 148726, 154134, 167319, 171077, 171710, 172675, 172926, 173688, 175562]\n"
     ]
    }
   ],
   "source": [
    "def extract_findings_section(df):\n",
    "    empty_extractions = 0\n",
    "\n",
    "    extracted_findings_list = []\n",
    "    bad_indices = []\n",
    "\n",
    "    for i, report in enumerate(df[\"report_page1\"]):\n",
    "        match = re.search(r'(.+)(?:Measurements and Calculations:)', report, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            findings = match.group(1)\n",
    "            \n",
    "            # Remove study info line\n",
    "            match = re.search(r'Study Info:.*', findings, re.IGNORECASE)\n",
    "            if match:\n",
    "                findings = findings[:match.start()] + findings[match.end():]\n",
    "\n",
    "            # Remove comparison to previous exam line\n",
    "            match = re.search(r'Comparison to Previous Exam.*|Compared to prior exam.*|Compared to previous study.*', findings, re.IGNORECASE)\n",
    "            if match:\n",
    "                findings = findings[:match.start()] + findings[match.end():]\n",
    "\n",
    "            cleaned_lines = [line.strip() for line in findings.splitlines() if line.strip()]\n",
    "            \n",
    "            extracted_findings_list.append('\\n'.join(cleaned_lines))\n",
    "        else:\n",
    "            match2 = re.search(r'(.+)(?:Comparison to Previous Exam)|(.+)(?:Compared to prior exam)|(.+)(?:Compared to previous study)', report, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "            if match2:\n",
    "                findings = match2.group(1)\n",
    "                # Remove study info line\n",
    "                match = re.search(r'Study Info:.*', findings, re.IGNORECASE)\n",
    "                if match:\n",
    "                    findings = findings[:match.start()] + findings[match.end():]\n",
    "\n",
    "                cleaned_lines = [line.strip() for line in findings.splitlines() if line.strip()]\n",
    "                \n",
    "                extracted_findings_list.append('\\n'.join(cleaned_lines))\n",
    "            else:\n",
    "                empty_extractions += 1\n",
    "                extracted_findings_list.append(-1)\n",
    "                bad_indices.append(df['report_page1'].index[i])\n",
    "    \n",
    "    print(f\"Empty extractions: {empty_extractions} Extraction percentage: {empty_extractions/len(df)*100}\")\n",
    "    print(f\"Bad reports: {bad_indices}\")\n",
    "    df[\"extracted_findings\"] = extracted_findings_list\n",
    "\n",
    "extract_findings_section(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty extractions: 71 Extraction percentage: 0.03845278972281496\n",
      "Bad reports: [2385, 8450, 10295, 18240, 19888, 20265, 20872, 21786, 22032, 22825, 24556, 24851, 28956, 36989, 40182, 43473, 45364, 53577, 55679, 56322, 57300, 57554, 58374, 58375, 60274, 60596, 64890, 65524, 72177, 74071, 82190, 83027, 86264, 86675, 91990, 94037, 100366, 102916, 103851, 107422, 107868, 108541, 108788, 113166, 115015, 122902, 123725, 126900, 127295, 132497, 134500, 140691, 143195, 144117, 147632, 148069, 148726, 148972, 154134, 157254, 160289, 161944, 169186, 171077, 171710, 172675, 172926, 173688, 175562, 179771, 180329]\n"
     ]
    }
   ],
   "source": [
    "def extract_calculations_section(df):\n",
    "    empty_extractions = 0\n",
    "\n",
    "    extracted_calcs_list = []\n",
    "    bad_indices = []\n",
    "\n",
    "    for i, report in enumerate(df[\"report_page1\"]):\n",
    "        match = re.search(r'(?:Measurements and Calculations:)(.+?)(?:Sonographer|$)', report, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            findings = match.group(1)\n",
    "            cleaned_lines = [line.strip() for line in findings.splitlines() if line.strip()]\n",
    "            extracted_calcs_list.append('\\n'.join(cleaned_lines))\n",
    "        else:\n",
    "            match2 = re.search(r'(RVd\\s+A4C:.+?)(?:Sonographer|$)', report, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "            if match2:\n",
    "                findings = match2.group(1)\n",
    "                cleaned_lines = [line.strip() for line in findings.splitlines() if line.strip()]\n",
    "                extracted_calcs_list.append('\\n'.join(cleaned_lines))\n",
    "            else:\n",
    "                empty_extractions += 1\n",
    "                extracted_calcs_list.append(-1)\n",
    "                bad_indices.append(df['report_page1'].index[i])\n",
    "    \n",
    "    print(f\"Empty extractions: {empty_extractions} Extraction percentage: {empty_extractions/len(df)*100}\")\n",
    "    print(f\"Bad reports: {bad_indices}\")\n",
    "    df[\"extracted_calcs\"] = extracted_calcs_list\n",
    "\n",
    "extract_calculations_section(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of unusable reports: 78, patient_indices: [2385, 8450, 10295, 16181, 18240, 19888, 20265, 20872, 21786, 22032, 22825, 24556, 24851, 28956, 36989, 40182, 43473, 45364, 51432, 53577, 55679, 56322, 57300, 57554, 58374, 58375, 60274, 60596, 64890, 65524, 72177, 74071, 79225, 82190, 83027, 86264, 86675, 91990, 94037, 99678, 100366, 102916, 103851, 107422, 107868, 108541, 108788, 113166, 115015, 120014, 122902, 123725, 126900, 127295, 132497, 134500, 140015, 140691, 143195, 144117, 147632, 148069, 148726, 148972, 154134, 157254, 160289, 161944, 167319, 169186, 171077, 171710, 172675, 172926, 173688, 175562, 179771, 180329]\n"
     ]
    }
   ],
   "source": [
    "def determine_report_usability(df):\n",
    "    usable_list = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"extracted_calcs\"] != -1 and row[\"extracted_findings\"] != -1:\n",
    "            usable_list.append(True)\n",
    "        else:\n",
    "            usable_list.append(False)\n",
    "\n",
    "    df[\"data_usable\"] = usable_list\n",
    "\n",
    "def drop_unusable_reports(df):\n",
    "    indices =[]\n",
    "\n",
    "    for i, usable in enumerate(df[\"data_usable\"]):\n",
    "        if not usable:\n",
    "            indices.append(i)\n",
    "\n",
    "    df.drop(indices, axis=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True) \n",
    "    print(f'no. of unusable reports: {len(indices)}, patient_indices: {indices}')\n",
    "    \n",
    "\n",
    "determine_report_usability(df)\n",
    "drop_unusable_reports(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_calc_searches(example_text):\n",
    "    la_mm = re.search(r\"left\\s+atrium:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    ivsd = re.search(r\"ivsd:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    lvpwd = re.search(r\"lvpwd:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    la_vol_bpidx = re.search(r\"la\\s?(?:volume|vol)\\s?index\\s?(?:\\(Biplane\\))?:\\s+[*]?([\\d\\.]+)\\s+ml/m\", example_text, flags = re.IGNORECASE)\n",
    "    lvidd = re.search(r\"lvidd:\\s+[*]?(\\d+)\", example_text, flags = re.IGNORECASE)\n",
    "    lvidd_idx = re.search(r\"lvidd\\sindex:\\s+[*]?(\\d+)\\s+mm/m\", example_text, flags = re.IGNORECASE)\n",
    "    rvd_a4c = re.search(r\"rvd\\sa4c:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    lvids = re.search(r\"lvids:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    rv_s = re.search(r\"rv\\sS.\\s+[*]?([\\d\\.]+)\\s+cm/s\", example_text, flags = re.IGNORECASE)\n",
    "    lvef = re.search(r\"lv\\s?ef\\s?.(?:Biplane|Visual).:\\s+[*]?(\\d+)\\s?%?\", example_text, flags = re.IGNORECASE)\n",
    "    tapse = re.search(r\"tapse:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    lv_mass_idx = re.search(r\"lv\\smass\\sindex:\\s+[*]?([\\d\\.]+)\\s+g/m\", example_text, flags = re.IGNORECASE)   \n",
    "    lv_rwt = re.search(r\"lv\\s?rwt:\\s+[*]?([\\d\\.]+)\", example_text, flags = re.IGNORECASE)\n",
    "    ra_vol_idx = re.search(r\"ra\\s?vol[\\w|\\s]+index:\\s+[*]?(\\d+)\\s+ml/m\", example_text, flags = re.IGNORECASE)\n",
    "    lv_edv_idx = re.search(r\"lv\\s?edv\\s?index:\\s+[*]?([\\d\\.]+)\\s+ml/m\", example_text, flags = re.IGNORECASE)\n",
    "    lv_esv_idx = re.search(r\"lv\\s?esv\\s?index:\\s+[*]?([\\d\\.]+)\\s+ml/m\", example_text, flags = re.IGNORECASE)\n",
    "    aorta_sinuses = re.search(r\"aorta\\ssinuses:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    lvot_diam = re.search(r\"lvot\\sdiam:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    aorta_sinuses_idx = re.search(r\"aorta\\ssinuses\\sindex:\\s+[*]?([\\d\\.]+)\\s+mm/m\", example_text, flags = re.IGNORECASE)\n",
    "    prox_asc_aorta = re.search(r\"prox\\sascending\\saorta:\\s+[*]?(\\d+)\\s+mm\", example_text, flags = re.IGNORECASE)\n",
    "    prox_asc_aorta_idx = re.search(r\"prox\\sasc\\saorta\\sindex:\\s+[*]?([\\d\\.]+)\\s+mm/m\", example_text, flags = re.IGNORECASE)\n",
    "    mv_peak_e = re.search(r\"mv\\speak\\se:\\s+[*]?([\\d\\.]+)\\s+cm/s\", example_text, flags = re.IGNORECASE)\n",
    "    mv_peak_a = re.search(r\"mv\\speak\\sa:\\s+[*]?([\\d\\.]+)\\s+cm/s\", example_text, flags = re.IGNORECASE)\n",
    "    mv_ea_ratio = re.search(r\"mv\\se/a\\sratio:\\s+[*]?([\\d\\.]+)\", example_text, flags = re.IGNORECASE)\n",
    "    decel_time = re.search(r\"decel\\stime:\\s+[*]?(\\d+)\\s+msec\", example_text, flags = re.IGNORECASE)\n",
    "    lateral_e = re.search(r\"lateral\\se\\s?:\\s+[*]?([\\d\\.]+)\\s+cm/s\", example_text, flags = re.IGNORECASE)\n",
    "    septal_e = re.search(r\"septal\\se\\s?:\\s+[*]?([\\d\\.]+)\\s+cm/s\", example_text, flags = re.IGNORECASE)\n",
    "    avg_ee_ratio = re.search(r\"average\\se/e\\sratio:\\s+[*]?([\\d\\.]+)\", example_text, flags = re.IGNORECASE)\n",
    "    tr_max_velocity = re.search(r\"TR\\s+max\\s+velocity:\\s+[*]?([\\d\\.]+)\\s+m/s\", example_text, flags = re.IGNORECASE)\n",
    "    ra_pressure = re.search(r\"ra\\spressure:?\\s+[*]?([\\d\\.]+)\\s+mmHg\", example_text, flags = re.IGNORECASE)\n",
    "    pasp = re.search(r\"pasp:\\s+[*]?(\\d+)\\s+mmHg\", example_text, flags = re.IGNORECASE)\n",
    "\n",
    "\n",
    "    measurements = [la_mm, la_vol_bpidx,\n",
    "                   lvpwd, lvidd, lvidd_idx, lvef, lv_mass_idx, lv_rwt, lv_edv_idx, lv_esv_idx, lvot_diam,\n",
    "                   ivsd, lvids,\n",
    "                   rvd_a4c, rv_s,\n",
    "                    tapse,\n",
    "                   ra_vol_idx,\n",
    "                    aorta_sinuses, aorta_sinuses_idx, prox_asc_aorta, prox_asc_aorta_idx,\n",
    "                   mv_peak_e, mv_peak_a, mv_ea_ratio,\n",
    "                    decel_time,\n",
    "                    lateral_e, septal_e,\n",
    "                    avg_ee_ratio,\n",
    "                    tr_max_velocity,\n",
    "                    ra_pressure,\n",
    "                    pasp]\n",
    "    \n",
    "    measurements_ls = [m.group(1) if m !=None else -1 for m in measurements]\n",
    "    \n",
    "    return measurements_ls   \n",
    "\n",
    "print(run_calc_searches(df[\"extracted_calcs\"].iloc[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_scale(img, res=(224, 224), interpolation=cv2.INTER_CUBIC, zoom=0.1):\n",
    "    in_res = (img.shape[1], img.shape[0])\n",
    "    r_in = in_res[0] / in_res[1]\n",
    "    r_out = res[0] / res[1]\n",
    "\n",
    "    if r_in > r_out:\n",
    "        padding = int(round((in_res[0] - r_out * in_res[1]) / 2))\n",
    "        if padding > 0:\n",
    "            img = img[:, padding:-padding]\n",
    "    if r_in < r_out:\n",
    "        padding = int(round((in_res[1] - in_res[0] / r_out) / 2))\n",
    "        if padding > 0:\n",
    "            img = img[padding:-padding]\n",
    "    if zoom != 0:\n",
    "        pad_x = round(int(img.shape[1] * zoom))\n",
    "        pad_y = round(int(img.shape[0] * zoom))\n",
    "        if pad_y * 2 < img.shape[0] and pad_x * 2 < img.shape[1]:\n",
    "            img = img[pad_y:img.shape[0]-pad_y, pad_x:img.shape[1]-pad_x]\n",
    "\n",
    "    img = cv2.resize(img, res, interpolation=interpolation)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients: 34363, Val Patients: 86, Test Patients: 349\n"
     ]
    }
   ],
   "source": [
    "val_percent = 1/400\n",
    "test_percent = 1/100\n",
    "train_percent = 1 - val_percent - test_percent\n",
    "\n",
    "grouped = df.groupby(\"patient_id\").first().reset_index()\n",
    "patients = grouped[\"patient_id\"].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_index = int(train_percent * len(patients))\n",
    "val_index = int(val_percent * len(patients))\n",
    "\n",
    "train_patients = patients[:train_index]\n",
    "val_patients = patients[train_index:train_index + val_index]\n",
    "test_patients = patients[train_index + val_index:]\n",
    "\n",
    "print(f\"Train Patients: {len(train_patients)}, Val Patients: {len(val_patients)}, Test Patients: {len(test_patients)}\")\n",
    "\n",
    "df_train = df[df[\"patient_id\"].isin(train_patients)]\n",
    "df_val = df[df[\"patient_id\"].isin(val_patients)]\n",
    "df_test = df[df[\"patient_id\"].isin(test_patients)]\n",
    "\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    "#model_name = \"yikuan8/Clinical-Longformer\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_vid_pairs(df):\n",
    "\n",
    "    # Group by exam_id and get the first report per exam\n",
    "    grouped = df.groupby(\"exam_id\")[\"extracted_findings\"].first().reset_index()\n",
    "\n",
    "    batch_size = 32\n",
    "    report_dict = {}\n",
    "\n",
    "    exam_ids = grouped[\"exam_id\"].tolist()\n",
    "    reports = grouped[\"extracted_findings\"].tolist()\n",
    "\n",
    "    for i in tqdm(range(0, len(reports), batch_size)):\n",
    "        batch_reports = reports[i:i+batch_size]\n",
    "        batch_exam_ids = exam_ids[i:i+batch_size]\n",
    "\n",
    "        tokens = tokenizer(batch_reports, padding=False, truncation=False, add_special_tokens=False)\n",
    "\n",
    "        for j, input_ids in enumerate(tokens[\"input_ids\"]):\n",
    "            if len(input_ids) <= 512:\n",
    "                report_dict[batch_exam_ids[j]] = batch_reports[j]\n",
    "\n",
    "\n",
    "    # Build vid_dict using filtered exam_ids\n",
    "    vid_dict = {}\n",
    "    for exam in tqdm(report_dict.keys()):\n",
    "        vid_paths = df[df[\"exam_id\"] == exam][\"processed_file_address\"].tolist()\n",
    "        vid_dict[exam] = vid_paths\n",
    "\n",
    "    bad_vids_count = 0\n",
    "    good_vids_count = 0\n",
    "    min_number_batches = 0\n",
    "    # Group video paths by report\n",
    "    report_to_vids = defaultdict(list)\n",
    "    for exam, vids in tqdm(vid_dict.items()):\n",
    "\n",
    "        good_vids = []\n",
    "        for vid in vids:\n",
    "            vid_path = f'/data{vid[15:]}'\n",
    "\n",
    "            if os.path.exists(vid_path):\n",
    "                good_vids.append(vid_path)\n",
    "                good_vids_count += 1\n",
    "            else:\n",
    "                bad_vids_count += 1\n",
    "\n",
    "        if good_vids:\n",
    "            report = report_dict[exam]\n",
    "            report_to_vids[report].extend(good_vids)\n",
    "            \n",
    "        min_number_batches = max(min_number_batches, len(good_vids))\n",
    "\n",
    "    num_batches = max(math.ceil(good_vids_count/batch_size), min_number_batches)\n",
    "\n",
    "    batches = [[] for _ in range(num_batches)]\n",
    "    batch_lengths = [0] * num_batches\n",
    "\n",
    "    i = 0\n",
    "    for report, vids in tqdm(report_to_vids.items()):\n",
    "        for vid in vids:\n",
    "            attempts = 0\n",
    "            found_batch = False\n",
    "            while attempts < num_batches:\n",
    "                batch_idx = i % num_batches\n",
    "                if batch_lengths[batch_idx] < batch_size:\n",
    "                    batches[batch_idx].append((report, vid))\n",
    "                    batch_lengths[batch_idx] += 1\n",
    "                    found_batch = True\n",
    "                    break\n",
    "                i += 1\n",
    "                attempts += 1\n",
    "            if not found_batch:\n",
    "                raise ValueError(\"Unable to find batch for video\")\n",
    "            i += 1  # Move to next batch for next video of same report\n",
    "\n",
    "    batch_sizes = [len(batch) for batch in batches]\n",
    "\n",
    "    print(batch_sizes)\n",
    "    print(f\"Number of missing videos: {bad_vids_count}\")\n",
    "\n",
    "    # Flatten batches into final list\n",
    "    report_vid_pairs = [pair for batch in batches for pair in batch]\n",
    "\n",
    "    return report_vid_pairs, batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(path):\n",
    "    frames_to_take = 32\n",
    "    frame_stride = 2\n",
    "    video_size = 224\n",
    "\n",
    "    mean = torch.tensor([29.110628, 28.076836, 29.096405]).reshape(3, 1, 1, 1)\n",
    "    std = torch.tensor([47.989223, 46.456997, 47.20083]).reshape(3, 1, 1, 1)\n",
    "\n",
    "    mat_data = scipy.io.loadmat(path)\n",
    "    volume = np.array(mat_data['cropped'])\n",
    "    volume = crop_and_scale(volume)\n",
    "    volume = np.repeat(volume[..., None], 3, axis=3)\n",
    "    volume = np.transpose(volume, (3, 2, 0, 1))\n",
    "    x = torch.as_tensor(volume, dtype=torch.float)\n",
    "    x.sub_(mean).div_(std)\n",
    "\n",
    "    if x.shape[1] < frames_to_take:\n",
    "        padding = torch.zeros((3, frames_to_take - x.shape[1], video_size, video_size), dtype=torch.float)\n",
    "        x = torch.cat((x, padding), dim=1)\n",
    "\n",
    "    return x[:, :frames_to_take:frame_stride, :, :]\n",
    "\n",
    "\n",
    "def batch_generator(report_vid_pairs, batch_sizes):\n",
    "    idx = 0\n",
    "    for batch_size in batch_sizes:\n",
    "        batch = report_vid_pairs[idx:idx + batch_size]\n",
    "        idx += batch_size\n",
    "\n",
    "        # Process this specific batch\n",
    "        reports = [r for r, _ in batch]\n",
    "        paths = [v for _, v in batch]\n",
    "\n",
    "        padded = tokenizer(reports, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids = padded[\"input_ids\"]\n",
    "        attention_mask = padded[\"attention_mask\"]\n",
    "        token_type_ids = padded.get(\"token_type_ids\")\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            videos = list(executor.map(process_path, paths))\n",
    "        video_tensor = torch.stack(videos)\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            yield video_tensor, input_ids, attention_mask, token_type_ids\n",
    "        else:\n",
    "            yield video_tensor, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.97it/s]\n",
      "100%|██████████| 394/394 [00:00<00:00, 1630.83it/s]\n",
      "100%|██████████| 394/394 [00:04<00:00, 81.08it/s] \n",
      "100%|██████████| 367/367 [00:00<00:00, 191408.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "Number of missing videos: 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.79it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 1655.21it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 106.96it/s]\n",
      "100%|██████████| 87/87 [00:00<00:00, 140618.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31]\n",
      "Number of missing videos: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [06:47<00:00,  2.92it/s]\n",
      "100%|██████████| 37889/37889 [00:26<00:00, 1435.74it/s]\n",
      "100%|██████████| 37889/37889 [05:12<00:00, 121.32it/s]\n",
      "100%|██████████| 35301/35301 [00:00<00:00, 168861.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "Number of missing videos: 20322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_report_vid_pairs, test_batch_sizes = get_report_vid_pairs(df_test)\n",
    "\n",
    "val_report_vid_pairs, val_batch_sizes = get_report_vid_pairs(df_val)\n",
    "\n",
    "train_report_vid_pairs, train_batch_sizes = get_report_vid_pairs(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 4e-5\n",
    "weight_decay = 1e-6\n",
    "num_epochs = 60\n",
    "save_path = \"/checkpoints/best_model.pt\"\n",
    "data_path = os.path.expanduser(\"~/model_data/weights\")\n",
    "cuda_device = 1\n",
    "\n",
    "DEVICE = torch.device(f'cuda:{cuda_device}' if torch.cuda.is_available() else 'cpu')\n",
    "EMBED_DIM = 512\n",
    "TEXT_MODEL_FROZEN_LAYERS = 6\n",
    "TEXT_MODEL_NAME = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    "\n",
    "# === Load Echo Encoder ===\n",
    "checkpoint = torch.load(os.path.join(data_path, \"echo_prime_encoder.pt\"), map_location=DEVICE)\n",
    "echo_encoder = torchvision.models.video.mvit_v2_s()\n",
    "echo_encoder.head[-1] = nn.Linear(echo_encoder.head[-1].in_features, EMBED_DIM)\n",
    "echo_encoder.load_state_dict(checkpoint)\n",
    "echo_encoder.eval()\n",
    "echo_encoder.to(DEVICE)\n",
    "for param in echo_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "text_encoder_full = AutoModel.from_pretrained(TEXT_MODEL_NAME)\n",
    "for layer in text_encoder_full.encoder.layer[:TEXT_MODEL_FROZEN_LAYERS]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, base_encoder, embed_dim):\n",
    "        super().__init__()\n",
    "        self.base_encoder = base_encoder\n",
    "        self.projector = nn.Linear(base_encoder.config.hidden_size, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.base_encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.projector(pooled)\n",
    "\n",
    "text_encoder = TextEncoder(text_encoder_full, EMBED_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.35 GiB. GPU 1 has a total capacity of 15.77 GiB of which 717.12 MiB is free. Process 2089 has 15.07 GiB memory in use. Of the allocated memory 7.42 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# if token_type_ids:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     token_type_ids.to(DEVICE)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     video_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mecho_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     video_embeds \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(video_embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m text_encoder(input_ids, attention_mask)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/mvit.py:558\u001b[0m, in \u001b[0;36mMViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    556\u001b[0m thw \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding\u001b[38;5;241m.\u001b[39mtemporal_size,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding\u001b[38;5;241m.\u001b[39mspatial_size\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 558\u001b[0m     x, thw \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# classifier \"token\" as used by standard language architectures\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/mvit.py:384\u001b[0m, in \u001b[0;36mMultiscaleBlock.forward\u001b[0;34m(self, x, thw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, thw: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    383\u001b[0m     x_norm1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_transposal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 384\u001b[0m     x_attn, thw_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_after_attn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(x_norm1)\n\u001b[1;32m    386\u001b[0m     x_skip \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_skip(x, thw)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/mvit.py:303\u001b[0m, in \u001b[0;36mMultiscaleAttention.forward\u001b[0;34m(self, x, thw)\u001b[0m\n\u001b[1;32m    301\u001b[0m attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m*\u001b[39m q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43m_add_rel_pos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    314\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn, v)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/mvit.py:175\u001b[0m, in \u001b[0;36m_add_rel_pos\u001b[0;34m(attn, q, q_thw, k_thw, rel_pos_h, rel_pos_w, rel_pos_t)\u001b[0m\n\u001b[1;32m    168\u001b[0m rel_q_t \u001b[38;5;241m=\u001b[39m rel_q_t\u001b[38;5;241m.\u001b[39mview(B, n_head, q_h, q_w, q_t, k_t)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Combine rel pos.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m rel_pos \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrel_h_q\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_w_q\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_q_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[0;32m--> 175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Add it to attention\u001b[39;00m\n\u001b[1;32m    178\u001b[0m attn[:, :, \u001b[38;5;241m1\u001b[39m:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rel_pos\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.35 GiB. GPU 1 has a total capacity of 15.77 GiB of which 717.12 MiB is free. Process 2089 has 15.07 GiB memory in use. Of the allocated memory 7.42 GiB is allocated by PyTorch, and 6.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    " # === Loss & Optimizer ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(text_encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# === Training Loop ===\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    text_encoder.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in batch_generator(train_report_vid_pairs, train_batch_sizes):\n",
    "        if len(batch) == 4:\n",
    "            video_tensor, input_ids, attention_mask, token_type_ids = batch\n",
    "        else:\n",
    "            video_tensor, input_ids, attention_mask = batch\n",
    "            token_type_ids = None\n",
    "\n",
    "        video_tensor = video_tensor.to(DEVICE)\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "\n",
    "        # if token_type_ids:\n",
    "        #     token_type_ids.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            video_embeds = echo_encoder(video_tensor)\n",
    "            video_embeds = F.normalize(video_embeds, dim=1)\n",
    "\n",
    "        text_embeds = text_encoder(input_ids, attention_mask)\n",
    "        text_embeds = F.normalize(text_embeds, dim=1)\n",
    "\n",
    "        sim_matrix = torch.matmul(text_embeds, video_embeds.T)\n",
    "        target = torch.arange(batch_size).to(DEVICE)\n",
    "        loss = criterion(sim_matrix, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / 100\n",
    "\n",
    "    # === Validation ===\n",
    "    text_encoder.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator(val_report_vid_pairs, val_batch_sizes):\n",
    "            if len(batch) == 4:\n",
    "                video_tensor, input_ids, attention_mask, token_type_ids = batch\n",
    "            else:\n",
    "                video_tensor, input_ids, attention_mask = batch\n",
    "                token_type_ids = None\n",
    "            \n",
    "            video_tensor = video_tensor.to(DEVICE)\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "\n",
    "            video_embeds = echo_encoder(video_tensor)\n",
    "            video_embeds = F.normalize(video_embeds, dim=1)\n",
    "\n",
    "            text_embeds = text_encoder(input_ids, attention_mask)\n",
    "            text_embeds = F.normalize(text_embeds, dim=1)\n",
    "\n",
    "            sim_matrix = torch.matmul(text_embeds, video_embeds.T)\n",
    "            target = torch.arange(batch_size).to(DEVICE)\n",
    "            loss = criterion(sim_matrix, target)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / 20\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(text_encoder.state_dict(), save_path)\n",
    "        print(f\"✅ Saved new best model at epoch {epoch + 1} with val loss {best_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
